{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f0b1b88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/admin1/anaconda3/lib/python3.9/site-packages (3.5.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/admin1/anaconda3/lib/python3.9/site-packages (from spacy) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/admin1/anaconda3/lib/python3.9/site-packages (from spacy) (21.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/admin1/anaconda3/lib/python3.9/site-packages (from spacy) (1.22.4)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/admin1/anaconda3/lib/python3.9/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/admin1/anaconda3/lib/python3.9/site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: jinja2 in /home/admin1/anaconda3/lib/python3.9/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/admin1/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/admin1/anaconda3/lib/python3.9/site-packages (from spacy) (8.1.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/admin1/anaconda3/lib/python3.9/site-packages (from spacy) (1.10.4)\n",
      "Requirement already satisfied: setuptools in /home/admin1/anaconda3/lib/python3.9/site-packages (from spacy) (58.0.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/admin1/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/admin1/anaconda3/lib/python3.9/site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/admin1/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/admin1/anaconda3/lib/python3.9/site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/admin1/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/admin1/anaconda3/lib/python3.9/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/admin1/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/admin1/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/admin1/anaconda3/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/admin1/anaconda3/lib/python3.9/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/admin1/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/admin1/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/admin1/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/admin1/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/admin1/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/admin1/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/admin1/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/admin1/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/admin1/anaconda3/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/admin1/anaconda3/lib/python3.9/site-packages (from jinja2->spacy) (1.1.1)\n",
      "Requirement already satisfied: gensim in /home/admin1/anaconda3/lib/python3.9/site-packages (4.3.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/admin1/anaconda3/lib/python3.9/site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/admin1/anaconda3/lib/python3.9/site-packages (from gensim) (1.22.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/admin1/anaconda3/lib/python3.9/site-packages (from gensim) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "be0fba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import (word_tokenize,\n",
    "                          sent_tokenize,\n",
    "                          TreebankWordTokenizer,\n",
    "                          WordPunctTokenizer,\n",
    "                          TweetTokenizer,\n",
    "                          MWETokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed14480",
   "metadata": {},
   "source": [
    "## White Space Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1388dd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize Text: ['I', 'hope', \"I'd\", 'get', 'better', 'at', 'coding', 'and', 'land', 'a', 'decent', 'job.', 'Oh', 'no!', 'Anyways.', 'üòó', 'üßê']\n",
      "Lenght: 17\n"
     ]
    }
   ],
   "source": [
    "text = \"I hope I'd get better at coding and land a decent job. Oh no! Anyways. üòó üßê\"\n",
    "print(\"Tokenize Text: {}\".format(text.split(\" \")))\n",
    "print(\"Lenght: {}\".format(len(text.split(\" \"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1ec3dd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Words: ['I', 'hope', \"I'd\", 'get', 'better', 'at', 'coding', 'and', 'land', 'a', 'decent', 'job.', 'Oh', 'no!', 'Anyways.', 'üòó', 'üßê']\n",
      "Length: 17\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "WT = WhitespaceTokenizer()\n",
    "print(\"Tokenized Words: {}\".format(WT.tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01203211",
   "metadata": {},
   "source": [
    "## Words and Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4612fd",
   "metadata": {},
   "source": [
    "**Rules**\n",
    "1. Words - Break text based on whitespaces and punctuation.<br>\n",
    "2. Sentence - Break text based on punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cd0a7e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Words: ['I', 'hope', 'I', \"'d\", 'get', 'better', 'at', 'coding', 'and', 'land', 'a', 'decent', 'job', '.', 'Oh', 'no', '!', 'Anyways', '.', 'üòó', 'üßê']\n",
      "Length: 21\n"
     ]
    }
   ],
   "source": [
    "text = \"I hope I'd get better at coding and land a decent job. Oh no! Anyways. üòó üßê\"\n",
    "print(\"Tokenized Words: {}\".format(word_tokenize(text)))\n",
    "print(\"Length: {}\".format(len(word_tokenize(text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2f63b4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: [\"I hope I'd get better at coding and land a decent job.\", 'Oh no!', 'Anyways.', 'üòóüßê']\n",
      "Lenght: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenized Sentence: {}\".format(sent_tokenize(text)))\n",
    "print(\"Lenght: {}\".format(len(sent_tokenize(text))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505ca06b",
   "metadata": {},
   "source": [
    "## Punctuation Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a4d8e8",
   "metadata": {},
   "source": [
    "**Rules**\n",
    "1. Punctuation: Splits almost all special symbols and treat them as separate units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9ed864e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Punctuation: ['I', 'hope', 'I', \"'\", 'd', 'get', 'better', 'at', 'coding', 'and', 'land', 'a', 'decent', 'job', '.', 'Oh', 'no', '!', 'Anyways', '.', 'üòóüßê']\n",
      "Length: 21\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenized Punctuation: {}\".format(WordPunctTokenizer().tokenize(text)))\n",
    "print(\"Length: {}\".format(len(WordPunctTokenizer().tokenize(text))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ba722d",
   "metadata": {},
   "source": [
    "## Treebank Word Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aa32e4",
   "metadata": {},
   "source": [
    "**Rules**\n",
    "1. Treebank: Uses regular expressions to tokenize text.<br><br>\n",
    "Regular Expressions: A filter that describes a set of strings that matches the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a3e36fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Treebank: ['I', 'hope', 'I', \"'d\", 'get', 'better', 'at', 'coding', 'and', 'land', 'a', 'decent', 'job.', 'Oh', 'no', '!', 'Anyways.', 'üòóüßê']\n",
      "Length: 18\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenized Treebank: {}\".format(TreebankWordTokenizer().tokenize(text)))\n",
    "print(\"Length: {}\".format(len(TreebankWordTokenizer().tokenize(text))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350afb67",
   "metadata": {},
   "source": [
    "## Tweet Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2907ff83",
   "metadata": {},
   "source": [
    "**Rules**\n",
    "1. Tweet - Considers Emoji/Unicodes as different tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "662c8bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Tweet: [\"Don't\", 'take', 'everything', 'seriously', 'on', 'the', 'internet', 'üòó', 'üßê']\n",
      "Length: 9\n"
     ]
    }
   ],
   "source": [
    "text = \"Don't take everything seriously on the internet üòóüßê\"\n",
    "print(\"Tokenized Tweet: {}\".format(TweetTokenizer().tokenize(text)))\n",
    "print(\"Length: {}\".format(len(TweetTokenizer().tokenize(text))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56744fc",
   "metadata": {},
   "source": [
    "## MWET Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c97c943",
   "metadata": {},
   "source": [
    "MWET - Multi-Word Expression Tokenizer<br><br>\n",
    "**Rules**\n",
    "1. MWET - Allows the user to enter multiple word expressions before using the tokenizer on the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b27023ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized MWET: ['Transformers', '2', 'is', 'the', 'best', 'Transformer', 'movie!']\n",
      "Length: 45\n"
     ]
    }
   ],
   "source": [
    "text = \"Transformers 2 is the best Transformer movie!\"\n",
    "print(\"Tokenized MWET: {}\".format(MWETokenizer().tokenize(WT.tokenize(text))))\n",
    "print(\"Length: {}\".format(len(MWETokenizer().tokenize((text)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3ffd56ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized MWET: ['Transformers_2', 'is', 'the', 'best', 'Transformer', 'movie', '!']\n",
      "Lenght: 7\n"
     ]
    }
   ],
   "source": [
    "tokenizer = MWETokenizer()\n",
    "tokenizer.add_mwe(('Transformers', '2'))\n",
    "print(\"Tokenized MWET: {}\".format(tokenizer.tokenize(word_tokenize(text))))\n",
    "print(\"Lenght: {}\".format(len(tokenizer.tokenize(word_tokenize(text)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5706433",
   "metadata": {},
   "source": [
    "## Gensim Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "de668fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Gensim Word: ['I', 'hope', 'I', 'd', 'get', 'better', 'at', 'coding', 'and', 'land', 'a', 'decent', 'job', 'Oh', 'no', 'Anyways']\n",
      "Length: 16\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import tokenize\n",
    "text = \"I hope I'd get better at coding and land a decent job. Oh no! Anyways.\"\n",
    "print(\"Tokenized Gensim Word: {}\".format(list(tokenize(text))))\n",
    "print(\"Length: {}\".format(len(list(tokenize(text)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0344b7f",
   "metadata": {},
   "source": [
    "## Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bff85656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenizStemminge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ef16e2",
   "metadata": {},
   "source": [
    "**1. Individual Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ae4218df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program  :  program\n",
      "programs  :  program\n",
      "programmer  :  programm\n",
      "programming  :  program\n",
      "programmers  :  programm\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    " \n",
    "# choose some words to be stemmed\n",
    "words = [\"program\", \"programs\", \"programmer\", \"programming\", \"programmers\"]\n",
    " \n",
    "for w in words:\n",
    "    print(w, \" : \", ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee7fb8d",
   "metadata": {},
   "source": [
    "**2. Sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d46d17e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Programmers  :  programm\n",
      "program  :  program\n",
      "with  :  with\n",
      "programming  :  program\n",
      "languages  :  languag\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Programmers program with programming languages\"\n",
    "words = word_tokenize(sentence)\n",
    " \n",
    "for w in words:\n",
    "    print(w, \" : \", ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9fa090",
   "metadata": {},
   "source": [
    "**3. Stemming in Sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fe857cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " programm program with program languag\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "sentence = \"Programmers program with programming languages\"\n",
    "words = word_tokenize(sentence)\n",
    " \n",
    "# using reduce to apply stemmer to each word and join them back into a string\n",
    "stemmed_sentence = reduce(lambda x, y: x + \" \" + ps.stem(y), words, \"\")\n",
    " \n",
    "print(stemmed_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd61c58a",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e06ca7",
   "metadata": {},
   "source": [
    "Involves methods to identify and transform words into their base or root forms.<br><br>\n",
    "**1. Rule-Based**<br>\n",
    "Word: ‚Äúwalked‚Äù<br>\n",
    "Rule Application: Remove ‚Äú-ed‚Äù<br>\n",
    "Result: ‚Äúwalk\"<br><br>\n",
    "**2. Dictionary-Based**<br>\n",
    "‚Äòrunning‚Äô -> ‚Äòrun‚Äô<br>\n",
    "‚Äòbetter‚Äô -> ‚Äògood‚Äô<br>\n",
    "‚Äòwent‚Äô -> ‚Äògo‚Äô<br><br>\n",
    "**3. Machine Learning-Based**<br>\n",
    "Machine learning-based lemmatization leverages computational models to automatically learn the relationships between words and their base forms. Unlike rule-based or dictionary-based approaches, machine learning models, such as neural networks or statistical models, are trained on large text datasets to generalize patterns in language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "58cbfd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    " \n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bc4aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
